{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # request accesing pages\n",
    "from bs4 import BeautifulSoup # to scrape html code from pages\n",
    "import re # to cleanup some data points - searches count-\n",
    "from __future__ import division # needed for division\n",
    "import pandas as pd # to create a data frame\n",
    "import numpy as np # generate random times for pauses between requests \n",
    "import time # package for pausing code\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing searches & keywords\n",
    "\n",
    "searches = pd.read_csv('searches.csv')\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "\n",
    "# Defining a function to transform the desired keywords from a dataframe into a dictionary\n",
    "def dataframeToDict(df):\n",
    "    ncol = df.columns\n",
    "    skilldict = {}\n",
    "    for c in ncol:\n",
    "        cvalues = df[c].values\n",
    "        skilldict[c] = cvalues[~pd.isnull(cvalues)]\n",
    "    return skilldict\n",
    "\n",
    "keywords = dataframeToDict(keywords)\n",
    "seperator = \"_\" \n",
    "\n",
    "start_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Scraping job links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p1_start = time.time()\n",
    "\n",
    "# Creating the Data Frame that will hold the scraped data\n",
    "df_final = pd.DataFrame(columns=[\"Search\", \"Title\",\"Location\",\"Company\",\"Salary\", \"Summary\", \"Post_Date\", \"Link\"])\n",
    "print \"We are going to scrape jobs for \" + str(searches.shape[0]) + \" searches:\"\n",
    "print \" \"\n",
    "failed = 0\n",
    "for search in range(0,searches.shape[0], 1):\n",
    "    df = pd.DataFrame(columns=[\"Search\", \"Title\",\"Location\",\"Company\",\"Salary\", \"Summary\", \"Post_Date\", \"Link\"])\n",
    "    url = \"https://www.indeed.com/jobs?q=\" + searches.job_title[search].lower().replace(' ', '+') + \\\n",
    "    \"&l=\" + searches.job_location[search].lower().replace(' ', '+') + \"&\" + 'radius=' + str(searches.search_radius[search]) + \"&jt=\" + searches.job_type[search] + \\\n",
    "    \"&filter=0&limit=50\"\n",
    "    # making sure job title does not have \" \"\n",
    "    searches.job_title[search] = searches.job_title[search].replace('\"', '')\n",
    "    \n",
    "    # request and scrape first page results\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    # scraping the total number of jobs for this search\n",
    "    try:\n",
    "        results_count = soup.select('#searchCount')\n",
    "        # cleaning up the scraped string\n",
    "        results_count = re.sub('\\D', '', str(results_count))[1::]\n",
    "        results_count = int(results_count)\n",
    "    except:\n",
    "        results_count = 'NO'\n",
    "        failed = failed + 1\n",
    "\n",
    "    \n",
    "    print \"-------------------------\"\n",
    "    print \"\"\n",
    "    if results_count > 1000 and results_count <> \"NO\":\n",
    "        print \"There are \" + str(results_count) +  \" jobs for your [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!, but we can only scrape 1000.\"\n",
    "    else:\n",
    "        print \"There are \" + str(results_count) +  \" jobs for your [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!\"\n",
    "    \n",
    "    print \"\"\n",
    "    print \"link of this search: \" + url\n",
    "    print \"\"\n",
    "    \n",
    "\n",
    "    # to make sure we only scrape the exact number of job posts, otherwise we will have many duplicates\n",
    "\n",
    "    if results_count < 1000:\n",
    "        target = results_count\n",
    "    else:\n",
    "        target = 1000\n",
    "\n",
    "    # Loop to request different results pages\n",
    "    if results_count <> 'NO':\n",
    "\n",
    "        for page in range(0, target, 50):\n",
    "            if page is not 0:\n",
    "                progress =  int(page/target * 100)\n",
    "                print \"Scraped \" + str(page) + \" jobs [ \" + str(progress) + \"% ]  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ]\"\n",
    "\n",
    "            else:\n",
    "\n",
    "                print \"Initializing Scraping \" + str(target) + \" \" + str(searches.job_title[search]) +\" jobs in \" + searches.job_location[search] + \"!\" + \\\n",
    "                \"  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ]\"\n",
    "                print \" \"\n",
    "            new_url = url + \"&start=\" + str(page)\n",
    "            html = requests.get(new_url)\n",
    "            soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "        # Loop to scrape job title, location, company, salary, synopsis, post date, link\n",
    "\n",
    "            for each in soup.find_all(class_= \"result\"):\n",
    "                sponsor = each.find(class_='sponsoredGray') \n",
    "\n",
    "                if sponsor is None: #ignore any post that is not organic (sponsored)\n",
    "\n",
    "                    try:\n",
    "                        date = each.find('span', {'class':'date' }).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        date = 'None'\n",
    "                    try:\n",
    "                        joblink = each.find(class_= 'turnstileLink').get('href')\n",
    "                        link = 'https://www.indeed.com' + joblink\n",
    "                    except:\n",
    "                        link = 'None'\n",
    "                    try: \n",
    "                        title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "                    except:\n",
    "                        title = 'None'\n",
    "                    try:\n",
    "                        location = each.find('span', {'class':'location' }).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        location = 'None'\n",
    "                    try: \n",
    "                        company = each.find(class_='company').text.replace('\\n', '')\n",
    "                    except:\n",
    "                        company = 'None'\n",
    "                    try:\n",
    "                        salary = each.find('span', {'class':'no-wrap'}).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        salary = 'None'\n",
    "                    try:\n",
    "                        summary = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "                    except:\n",
    "                        summary = 'None'\n",
    "        # Append the scraped datapoints into a record in the Data Frame           \n",
    "\n",
    "                    df = df.append({'Title':title, 'Location':location, 'Company':company,\n",
    "                                    'Salary':salary, 'Summary':summary, 'Post_Date':date, 'Link':link,\n",
    "                                    'Search':searches.job_title[search]}, ignore_index=True)\n",
    "\n",
    "        # Pausing the loop for a random value in a normal distribution with a mean of 1 seconds, imitate human browsing     \n",
    "\n",
    "            pausetime = int(np.abs(np.random.randn(1) + 1))  \n",
    "            if page <> 0:\n",
    "                print \"Pausing for \" + str(pausetime) + \" seconds\"\n",
    "            print \"-------------------------\"\n",
    "            time.sleep(pausetime)\n",
    "    else:\n",
    "        print \"Skipping  [  \" + str(search + 1) + \" / \" +  str(str(searches.shape[0])) + \"  ] search!\"\n",
    "    df_final = pd.concat([df_final,df])\n",
    "    progress =  int(df.shape[0]/target * 100)  \n",
    "\n",
    "    if results_count <> \"NO\":\n",
    "        print \" \"\n",
    "        print \"Finished Scraping \" + str(df.shape[0]) + \" jobs [ \" + str(progress) + \"% ]  [  \" + str(search + 1) + \" / \" +  str(searches.shape[0]) + \"  ]  [  Total: \" + str(df_final.shape[0])  + \" jobs  ]\" \n",
    "        print \" \"\n",
    "    \n",
    "        if df.shape[0] <> target:\n",
    "            print \"Dont worry if the % or number of jobs dont match, Indeed changes the search results on the go as you browse!\"\n",
    "    \n",
    "    print \" \"\n",
    "print \" \"\n",
    "print \" \"\n",
    "print \"-------------------------\"\n",
    "print \"Done with the first phase\"\n",
    "scraped = df_final.shape[0]\n",
    "if failed <> 0:\n",
    "    print \"There were \" + str(failed) + \" searches that had no results, so we only scraped jobs for \" + str(searches.shape[0] - failed) + \" out of the \" +  str(searches.shape[0]) + \" searches.\" \n",
    "print \"Total jobs scraped: \" + str(scraped) + \" jobs.\"\n",
    "\n",
    "df_final.drop_duplicates(keep='first', subset=\"Link\", inplace=True)\n",
    "\n",
    "duplicates  = scraped - df_final.shape[0]\n",
    "\n",
    "print \"There were \" + str(duplicates) + \" duplicated jobs that we removed.\" \n",
    "print \"So we only saved \" + str(df_final.shape[0]) + \" jobs.\"\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "df1 = df_final\n",
    "\n",
    "p1_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p1_start))\n",
    "print \"Duration: \" + p1_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Scraping the body of Job Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df1\n",
    "\n",
    "p2_start = time.time()\n",
    "\n",
    "# Loop to request job links collected from the previous phase\n",
    "target_body = df_final.shape[0]\n",
    "df_body = pd.DataFrame(columns=[\"Body\"])\n",
    "\n",
    "for job_link in range(0, target_body, 1):\n",
    "    job_url = df_final.Link.iloc[job_link]\n",
    "\n",
    "    if job_link is not 0:\n",
    "        job_progress =  int(job_link/target_body * 100)\n",
    "        print \"Scraped the Body of \" + str(job_link) + \" jobs [ \" + str(job_progress) + \"% ]  [ \" + str(job_link) + \" / \" +  str(target_body) + \" ]\"\n",
    "\n",
    "    else:\n",
    "        print \"Initializing Scraping the Body of \" + str(target_body) + \" jobs!\"\n",
    "        print \" \"\n",
    "    html = requests.get(job_url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "\n",
    "    # Scrape the body of job link\n",
    "\n",
    "    try:\n",
    "        body = soup.select('.snip')\n",
    "        idx = body[0].getText().find('save job')\n",
    "        body = body[0].getText()[0:idx]\n",
    "    except:\n",
    "        \n",
    "        body = \"None\"\n",
    "\n",
    "    # Append the scraped body in a Data Frame           \n",
    "\n",
    "    df_body = df_body.append({'Body': body}, ignore_index=True)\n",
    "\n",
    "    # Pausing the loop for a random value in a normal distribution with a mean of 1 seconds, imitate human browsing     \n",
    "\n",
    "    pausetime = int(np.abs(np.random.randn(1) + 0))  \n",
    "    if job_link <> 0:\n",
    "        print \"Pausing for \" + str(pausetime) + \" seconds\"\n",
    "\n",
    "        print \"-------------------------\"\n",
    "        time.sleep(pausetime)\n",
    "\n",
    "progress =  int((job_link+1)/target_body * 100)\n",
    "\n",
    "print \" \"\n",
    "print \"Finished Scraping \" + str(df_final.shape[0]) + \" jobs [ \" + str(progress) + \"% ]\"\n",
    "print \" \"\n",
    "print \"-------------------------\"\n",
    "print \"-------------------------\"\n",
    "print \" \"\n",
    "print \" \"\n",
    "# Merging the old and new Data Frames\n",
    "\n",
    "df_final = pd.merge(df_final, df_body, left_index=True, right_index=True)\n",
    "\n",
    "scraped = df_final.shape[0]\n",
    "\n",
    "print \"Total jobs scraped: \" + str(scraped) + \" jobs.\"\n",
    "\n",
    "\n",
    "df_final.drop_duplicates(keep='first', subset=\"Body\", inplace=True)\n",
    "\n",
    "duplicates  = scraped - df_final.shape[0]\n",
    "\n",
    "print \"We found \" + str(duplicates) + \" duplicated jobs that we removed.\" \n",
    "print \"So we only saved \" + str(df_final.shape[0]) + \" jobs.\"\n",
    "\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p2_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p2_start))\n",
    "\n",
    "df2 = df_final\n",
    "print \"Duration: \" + p2_duration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Text mining the body of Job Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df2\n",
    "\n",
    "p3_start = time.time()\n",
    "\n",
    "print \"Initializing extracting keywords from Body of \" + str(df_final.shape[0]) + \" jobs!\"\n",
    "print \" \"\n",
    "print \"-------------------------\"\n",
    "columns = []\n",
    "dictionary_count = len(keywords.keys())\n",
    "#creating columns for keywords\n",
    "for x in range(0, dictionary_count, 1):\n",
    "    \n",
    "    for y in range(1, len(keywords.get(list(keywords)[x])), 1):\n",
    "        keyword = keywords.get(list(keywords)[x])[0] + seperator + keywords.get(list(keywords)[x])[y]\n",
    "        columns.append(keyword)\n",
    "        \n",
    "df_exp = pd.DataFrame(columns= ['Experience', 'Exp Min', 'Exp Max'])\n",
    "\n",
    "df_text = pd.DataFrame(columns= [columns] , index=df_final.index)\n",
    "\n",
    "keyword_count = len(list(df_text.columns.values))\n",
    "\n",
    "#Experience\n",
    "for row in range(0, df_final.shape[0], 1):\n",
    "    try:\n",
    "        idx = df_final.Body[row].find('years')\n",
    "        experience = df_final.Body[row][idx-10:idx+5]\n",
    "        exp = re.sub('\\D', '', experience)\n",
    "        experience = df_final.Body[row][idx-35:idx+35]\n",
    "        if len(exp) > 4 or exp == \"\":\n",
    "            experience = None\n",
    "            expmin = None\n",
    "            expmax = None\n",
    "        else:\n",
    "            if len(exp) == 4:\n",
    "                expmin = exp[0:2]\n",
    "                expmax = exp[2:4]\n",
    "            if len(exp) == 3:\n",
    "                expmin = exp[0]\n",
    "                expmax = exp[1:3]\n",
    "            if len(exp) == 2:\n",
    "                if exp[1] > exp[0]:\n",
    "                    expmin = exp[0]\n",
    "                    expmax = exp[1]\n",
    "                else:\n",
    "                    expmin = exp\n",
    "                    expmax = None\n",
    "                \n",
    "            if len(exp) == 1:\n",
    "                expmin = exp\n",
    "                expmax = None\n",
    "            \n",
    "            if int(expmin) > 20:\n",
    "                expmin = None\n",
    "                expmax = None\n",
    "                experience = None\n",
    "            if expmax is not None and (int(expmax) > 20 or int(expmax) <= int(expmin)):\n",
    "                expmin = None\n",
    "                expmax = None\n",
    "                experience = None\n",
    "    except:\n",
    "        experience = None\n",
    "        expmin = None\n",
    "        expmax = None\n",
    "    \n",
    "    df_exp = df_exp.append({'Experience': experience, 'Exp Min': expmin, 'Exp Max' : expmax}, ignore_index=True)\n",
    "    \n",
    "#Keywords    \n",
    "    for key in range(0, keyword_count, 1):\n",
    "            word = list(df_text.columns.values)[key].split(\"_\")[1]\n",
    "            idx = df_final.Body[row].find(word)\n",
    "            \n",
    "            if word in df_final.Body.iloc[row] or (len(word) <> 2 and word.lower() in df_final.Body.iloc[row]) :\n",
    "                if df_final.Body.iloc[row][idx-1] is not \"H\":\n",
    "                    df_text.iloc[row,key] = 1            \n",
    "            else:\n",
    "                df_text.iloc[row,key] = 0 \n",
    "                \n",
    "    progress = int(row / (df_final.shape[0] - 1) * 100)\n",
    "    if (row+1) % 250 == 0:\n",
    "        print \"Extracted keywords from the Body of \" + str(row + 1) + \" jobs [ \" + str(progress) + \"% ]\" \n",
    "        print \"-------------------------\"\n",
    "\n",
    "print \" \"\n",
    "print \" \"\n",
    "print \"Finished extracting keywords from Body of \" + str(df_final.shape[0]) + \" jobs [ \" + str(progress) + \"% ]\"\n",
    "\n",
    "df_final = pd.merge(df_final, df_exp, left_index=True, right_index=True)\n",
    "df_final = pd.merge(df_final, df_text, left_index=True, right_index=True)\n",
    "\n",
    "p3_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p3_start))\n",
    "\n",
    "df3 = df_final\n",
    "print \"Duration: \" + p3_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Classifying jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final = df3\n",
    "\n",
    "p4_start = time.time()\n",
    "\n",
    "df_title = pd.DataFrame(columns= [\"General_Title\", \"Department\", \"City\"])\n",
    "df_final = df3\n",
    "for job in  range(0, df_final.shape[0], 1):\n",
    "    point = df_final.Title.iloc[job]\n",
    "    GT = ''\n",
    "    pre = ''\n",
    "    data = ''\n",
    "    post = 'Analyst'\n",
    "    dep = 'General'\n",
    "    if 'Senior ' in point or 'Sr' in point:\n",
    "        pre = 'Senior '\n",
    "#Data\n",
    "    if 'Data' in point:\n",
    "        data = 'Data '\n",
    "        dep = 'Statistics and Data Science'\n",
    "#Scientist\n",
    "    if 'Scientist' in point or 'Science' in point :\n",
    "        post = 'Scientist'\n",
    "        dep = 'Statistics and Data Science'\n",
    "        data = 'Data '\n",
    "#Business\n",
    "    if 'Business' in point or 'BI' in point or 'Intelligence' in point:\n",
    "        GT = 'Business '\n",
    "        dep = 'Business'\n",
    "        \n",
    "#IT\n",
    "    if 'IT' in point or 'System' in point or 'Security' in point or 'Engineer' in point or \\\n",
    "    'Technical' in point or 'Architect' in point or 'SOX' in point or 'Technology' in point:        \n",
    "        GT = 'IT '  \n",
    "        dep = 'IT'\n",
    "        \n",
    "#Marketing\n",
    "    if 'Marketing' in point or 'SEO' in point or 'SEM' in point or 'Campaign' in point or \\\n",
    "    'Product' in point or 'Digital' in point or 'Media' in point or 'Growth' in point or 'Engagement' in point:            \n",
    "        GT = 'Marketing '\n",
    "        dep = 'Marketing'\n",
    "        \n",
    "#Supply Chain    \n",
    "    if 'Supply Chain' in point or 'Logistics' in point or 'Operations' in point or 'Procurement' in point:        \n",
    "        GT = 'Supply Chain ' \n",
    "        dep = 'Supply Chain'\n",
    "        \n",
    "#Finance\n",
    "    if 'Finance' in point or 'Financial' in point or 'Asset' in point or 'Accounting' in point or \\\n",
    "    'Equity' in point or 'Investment' in point or 'Portfolio' in point or 'Banking' in point or \\\n",
    "    'Credit' in point or 'Risk' in point or 'Venture' in point or 'VC' in point or \\\n",
    "    'Securities' in point or 'Fund' in point or 'Investor' in point or 'Venture' in point or \\\n",
    "    'Capital' in point or 'Revenue' in point or 'Loan' in point or 'Wealth' in point or 'FinTech' in point or \\\n",
    "    'Tax' in point:\n",
    "        GT = 'Financial '\n",
    "        dep = 'Finance'\n",
    "#Sales\n",
    "    if 'Sales' in point or 'Operations' in point or 'Account' in point or 'Channel' in point or \\\n",
    "    'Partner' in point or 'Customer' in point or 'Relationship' in point or 'CRM'in point:\n",
    "        GT = 'Sales '\n",
    "        dep = 'Sales'\n",
    "#HR       \n",
    "    if 'HR' in point or 'Human Resources' in point or 'People' in point or 'Staff' in point or \\\n",
    "    'Organizational' in point or 'OD' in point or 'Talent' in point or 'Compensation' in point or \\\n",
    "    'Rewards' in point or 'Payroll' in point or 'Recruiting' in point or 'Benefit' in point:\n",
    "        GT = 'HR '\n",
    "        dep = 'HR'\n",
    "           \n",
    "#City\n",
    "    idx = df_final.Location.iloc[job].find(',')\n",
    "    city = df_final.Location.iloc[job][:idx]    \n",
    "    \n",
    "#Progress        \n",
    "    progress = int(job / (df_final.shape[0] - 1) * 100)\n",
    "\n",
    "    if job % 250 == 0:\n",
    "        print \"Filtered \" + str(job) + \" jobs [ \" + str(progress) + \"% ]\" \n",
    "        print \"-------------------------\"        \n",
    "\n",
    "    df_title = df_title.append({'General_Title':(pre + GT + data + post), 'Department': dep , 'City' : city}, ignore_index=True)\n",
    "\n",
    "df_final = pd.merge(df_title, df_final, left_index=True, right_index=True)\n",
    "\n",
    "print \" \"\n",
    "print \" \"\n",
    "print \"Finished filtering \" + str(job) + \" jobs [ \" + str(progress) + \"% ]\"\n",
    "print \"-------------------------\" \n",
    "\n",
    "p4_duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - p4_start))\n",
    "\n",
    "end_time = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "\n",
    "df4 = df_final\n",
    "print 'We started at ' + start_time\n",
    "print 'Phase 1 took ' + p1_duration\n",
    "print 'Phase 2 took ' + p2_duration\n",
    "print 'Phase 3 took ' + p3_duration\n",
    "print 'Phase 4 took ' + p4_duration\n",
    "print 'We finished at ' + end_time\n",
    "print 'The code ran for ' + total_time\n",
    "\n",
    "# export dataset\n",
    "file_date = strftime(\"%m-%d-%Y\", gmtime())\n",
    "df_final.to_excel('dataset-' + file_date +'.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
